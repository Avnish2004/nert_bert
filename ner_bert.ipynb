{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QT1ISRY5JJy",
        "outputId": "34787231-389e-4310-d492-9a8b5e21fad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load JSON data from a file\n",
        "with open('/content/data.json', 'r') as file:\n",
        "    dataset = json.load(file)\n",
        "\n",
        "def extract_labels(json_data):\n",
        "    labels = set()\n",
        "    for item in json_data:\n",
        "        for entity in item['entities']:\n",
        "            labels.add(f\"B-{entity['label']}\")\n",
        "            labels.add(f\"I-{entity['label']}\")\n",
        "    labels.add(\"O\")  # Add the 'O' label for non-entity tokens\n",
        "    return list(labels)\n",
        "\n",
        "label_set = extract_labels(dataset)\n",
        "num_labels = len(label_set)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "# Freeze all BERT layers except the classification layer\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define batch_size and learning rate\n",
        "batch_size = 16 # Consider reducing this if you still run out of memory\n",
        "learning_rate = 2e-5\n",
        "\n",
        "def tokenize_and_format_data(dataset, tokenizer, entity_types):\n",
        "    tokenized_data = []\n",
        "    max_length = 512  # Maximum length for BERT input sequences\n",
        "\n",
        "    for item in dataset:\n",
        "        text = item['text']\n",
        "        entities = item['entities']\n",
        "\n",
        "        # Tokenize the input text using the BERT tokenizer\n",
        "        encodings = tokenizer(text, is_split_into_words=False, truncation=True, padding='max_length', max_length=max_length, return_offsets_mapping=True)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'])\n",
        "\n",
        "        # Initialize labels for each token as 'O' (Outside)\n",
        "        labels = ['O'] * len(tokens)\n",
        "\n",
        "        # Map original text positions to token positions\n",
        "        word_ids = encodings['offset_mapping']\n",
        "\n",
        "        for entity in entities:\n",
        "            start, end, entity_type = entity['start'], entity['end'], entity['label']\n",
        "            # Find token indices for the entity span\n",
        "            start_token = None\n",
        "            end_token = None\n",
        "            for idx, (orig_start, orig_end) in enumerate(word_ids):\n",
        "                if orig_start <= start < orig_end:\n",
        "                    start_token = idx\n",
        "                if orig_start < end <= orig_end:\n",
        "                    end_token = idx\n",
        "\n",
        "            if start_token is not None and end_token is not None:\n",
        "                labels[start_token] = f\"B-{entity_type}\"\n",
        "                for i in range(start_token + 1, end_token + 1):\n",
        "                    labels[i] = f\"I-{entity_type}\"\n",
        "\n",
        "        # Convert labels to IDs\n",
        "        label_ids = [entity_types.index(x) for x in labels]\n",
        "\n",
        "        # Pad label_ids to max_length\n",
        "        if len(label_ids) < max_length:\n",
        "            label_ids.extend([entity_types.index(\"O\")] * (max_length - len(label_ids)))\n",
        "        else:\n",
        "            label_ids = label_ids[:max_length]\n",
        "\n",
        "        tokenized_data.append({\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask'],\n",
        "            'labels': label_ids\n",
        "        })\n",
        "\n",
        "    # Convert tokenized data to PyTorch dataset\n",
        "    dataset = TensorDataset(\n",
        "        torch.tensor([item['input_ids'] for item in tokenized_data]),\n",
        "        torch.tensor([item['attention_mask'] for item in tokenized_data]),\n",
        "        torch.tensor([item['labels'] for item in tokenized_data])\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_data = tokenize_and_format_data(dataset, tokenizer, label_set)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
        "        # Move tensors to the same device as the model\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_dataloader)}\")\n",
        "\n",
        "    # Optional: Save model checkpoint\n",
        "    model.save_pretrained(f'checkpoint-{epoch + 1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEG9HYbs5pgf",
        "outputId": "2b27d00f-c5a7-4dd3-ab12-4a696e045dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 35/35 [22:57<00:00, 39.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 3.796078477587019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 35/35 [22:39<00:00, 38.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Loss: 3.6422064440590995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 35/35 [22:32<00:00, 38.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Loss: 3.4922445501599992\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 35/35 [22:50<00:00, 39.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Loss: 3.34906576020377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 35/35 [22:44<00:00, 38.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Loss: 3.2107688699449812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = './trained_model'\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_save_path = './tokenizer'\n",
        "tokenizer.save_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqvrobrw7EcC",
        "outputId": "a47c88ce-dc29-484a-e709-94640e29ae1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tokenizer/tokenizer_config.json',\n",
              " './tokenizer/special_tokens_map.json',\n",
              " './tokenizer/vocab.txt',\n",
              " './tokenizer/added_tokens.json',\n",
              " './tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification, BertTokenizerFast\n",
        "\n",
        "# Load the model\n",
        "model = BertForTokenClassification.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_save_path)"
      ],
      "metadata": {
        "id": "qi723PQXVeiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-BFgdRSVqe-",
        "outputId": "16a0cc96-d21a-4985-ebdb-a53373fccd48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=59, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define your input sentence\n",
        "sentence = \"symptoms of acute myocardial infarction.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "encodings = tokenizer(sentence, is_split_into_words=False, truncation=True, padding='max_length', max_length=512, return_offsets_mapping=True)\n",
        "input_ids = torch.tensor([encodings['input_ids']])\n",
        "attention_mask = torch.tensor([encodings['attention_mask']])"
      ],
      "metadata": {
        "id": "CQQ9_TtqVuS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Convert predictions to labels\n",
        "predicted_labels = [tokenizer.convert_ids_to_tokens(pred) for pred in predictions[0].tolist()]\n"
      ],
      "metadata": {
        "id": "_muULNVaVylk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = [label_set[label_id] for label_id in predictions[0].tolist()]"
      ],
      "metadata": {
        "id": "XAUGqECnV1oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'])\n",
        "for token, label in zip(tokens, predicted_labels):\n",
        "    print(f\"Token: {token}, Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNBPui4jV5ow",
        "outputId": "c0e10088-eff1-4924-b3da-ef9bbd7aeaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: [CLS], Label: I-PROCEDURE\n",
            "Token: symptoms, Label: O\n",
            "Token: of, Label: O\n",
            "Token: acute, Label: I-PROBLEM\n",
            "Token: my, Label: I-PROBLEM\n",
            "Token: ##oca, Label: I-PROBLEM\n",
            "Token: ##rdial, Label: I-PROBLEM\n",
            "Token: in, Label: O\n",
            "Token: ##far, Label: I-PROBLEM\n",
            "Token: ##ction, Label: O\n",
            "Token: ., Label: I-PROBLEM\n",
            "Token: [SEP], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: B-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: O\n",
            "Token: [PAD], Label: I-PROBLEM\n",
            "Token: [PAD], Label: I-PROBLEM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxEIE_yMV8az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}